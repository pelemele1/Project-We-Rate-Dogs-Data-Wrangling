{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report on Wrangling Efforts\n",
    "__by Felix Deichsel, August 23, 2021__\n",
    "\n",
    "In this document I summarize the wrangling efforts, that were made in order to obtain a clean dataframe, which can be used to  analyze the data in order to get some insights. The data, that was wrangled, comes from three differnt sources\n",
    "1. The WeRateDogs Twitter archive, that is downloaded from the udacity server\n",
    "2. The tweet image predictions data, which contains the breed of the dog in each tweet according to a neural network\n",
    "3. Data, that we get from the Twitter API using Python's Tweepy library, that is matching the tweet ids from the data in 1.\n",
    "\n",
    "Following assessement and cleaning steps were made\n",
    "\n",
    "## Archiv Data\n",
    "\n",
    "1. There were still included retweets in the archiv data. Since we are only interested in original tweets, I dropped all rows, that represent a retweet. Retweet rows are those rows, that contain a value in their retweeted_status_id column.\n",
    "\n",
    "2. There were still included replies in the archiv data. Since we are only interested in original tweets, I dropped all rows, that represent a reply. Reply rows are those rows, that contain a value in their in_reply_to_status_id column.\n",
    "\n",
    "3. I discovered, that there were some rows with a denominator unequal 10. In 5 cases I could correct them manually, because the wrong denominator was caused by wrong parsing.The other cases are due to by multiple dogs on the images and have been droped.\n",
    "\n",
    "4. The nominator columns also contained some questionable values. Some are from dog images with multiple dogs and some were parsed wrongly and some are just fun ratings (e.g. a dog got the rating 1776 according to date of independence day). I dropped all rows, that contained a rating numerator above 20, because values above 20 are not reasonable and also outliers.\n",
    "\n",
    "5. The parsing of the dog names in the archiv data was sometimes done incorrectly. In many cases an articel or adjective were parsed as name. Since all those wrong parsed values start with a lowercase, all those values were replaced with 'none'.\n",
    "\n",
    "6. For three rows there was no value in the expanded_url columns existent, as well as no link to the tweet in the text column. Those rows were dropped\n",
    "\n",
    "7. The timestamp column was of type object, which is not the approppriate datatype to analyze the data. Thus the type was changed to datetime.\n",
    "\n",
    "8. For each of the four diffent dog categories (puppo, doggo, floofer, pupper) there was one column of type boolean. They were melted to one column of type categorical, which contains the dog category. Since some tweets had two dog categories, I set the dog category for those rows to an empty string to eliminate any uncertainty.\n",
    "\n",
    "## Image prediction data\n",
    "1. The dog breed names in the image prediction data are written inconsistently (columns p1, p2, p3). The main issues are lower- and uppercase and -/_ for word separation. Thus all letters were changed to lowercase and I converted '-' to '_'.\n",
    "\n",
    "2. There were three different predictions for each image in the image prediction data. Since I am only interested on whether the image shows a dog and if yes, which dog breed is predicted, I condensed the whole dataset to two columns. The first is of type boolean and indicates, whether a dog could be predicted. If there is a dog, the second column contains the dog breed with the highest confidence niveau.\n",
    "\n",
    "3. There are also 324 tweets, where no dog at all could be predicted. The rows related to those images were dropped, cause the data analysis should deal only with tweets showing dogs.\n",
    "\n",
    "## API-Data\n",
    "1. The data from the twitter api was merged with the data from archiv and image prediction, because for each tweet id exaclty one row exists in each dataframe.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
